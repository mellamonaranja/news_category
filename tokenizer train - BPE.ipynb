{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:14:28.834149Z",
     "start_time": "2022-05-30T01:14:28.451814Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:14:28.839452Z",
     "start_time": "2022-05-30T01:14:28.835481Z"
    },
    "id": "ahc_PV2F4WxX"
   },
   "outputs": [],
   "source": [
    "def dump_jsonl(data, output_path, append=False):\n",
    "    mode = \"a+\" if append else \"w\"\n",
    "    with open(output_path, mode, encoding=\"utf-8\") as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + \"\\n\")\n",
    "\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    data = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip(\"\\n|\\r\")))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:14:28.847203Z",
     "start_time": "2022-05-30T01:14:28.841038Z"
    }
   },
   "outputs": [],
   "source": [
    "# '/home/jh/documents/datasets/git_category.json'\n",
    "os.chdir(\"/home/jh/documents\")\n",
    "\n",
    "vocab_size = 24000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoCW3M3_HeeV"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:14:28.852689Z",
     "start_time": "2022-05-30T01:14:28.848154Z"
    }
   },
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    #     text = re.sub(r' +', r' ', text.strip())\n",
    "    #     text = re.sub(r'(.{8,}?)\\1+', r'\\1', text)\n",
    "    #     text = re.sub(r'[^ ㄱ-ㅎㅏ-ㅣ가-힣A-Za-z0-9~!@#$%\\^\\&*\\(\\)_\\+\\-=\\[\\]{},\\./<>\\?]', r'', text)\n",
    "    #     text = re.sub(r'http.+', r'<url>', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.210012Z",
     "start_time": "2022-05-30T01:14:28.854491Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_jsonl(\"datasets/git_category.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.222231Z",
     "start_time": "2022-05-30T01:15:18.211604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서울박정호 SKT 대표이사가 25일 서울 중구 을지로 SKT타워에서 열린 SK텔레콤 제37기 정기 주주총회에 참석하고 있다SK텔레콤이 주주가치 제고를 위해 한국거래소에 자사주 869만주 소각을 반영한 변경상장을 완료했다고 14일 밝혔다 발행주식 총수의약 2조6000억원규모다 이로써 전날 기준 SK텔레콤의 발행주식 총수는 기존 8075만주에서 7206만주로 줄어들었다앞서 SK텔레콤은 지난 4일 올해 인적분할에 앞서 기업가치주주가치 제고를 위해 기존 자사주를 사실상 전량 소각한다고 발표한 바 있다 이번 자사주 소각으로 발행주식 총수가 감소하면서 기존 주주들의 지분율이 모두 상승했다SK텔레콤은 분할 후 기업가치가 올라갈 것으로 전망하는 증권업계 전반의 시각을 감안하면 기업 펀더멘털 변동없이 주식 수만 줄어든 상황이라며 자사주 소각 전보다 주식가치 상승 여력이 더 커졌다고 분석했다SK텔레콤은 발행주식 총수가 줄었음에도 전날 시가총액이 약 22조5000억원을 기록해 주주총회에서 기업구조인적분할 개편을 공식화했던 지난 3월25일 시가총액약 20조5000억원보다 10가량 늘었다고 설명했다 주가도 연초 대비 30 이상 상승했다고 덧붙였다한편 SK텔레콤은 상반기 내 이사회 의결을 거쳐 오는 10월 주주총회 11월 인적분할 법인의 재상장을 통해 인적분할 절차를 끝낼 계획이다'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.234332Z",
     "start_time": "2022-05-30T01:15:18.223606Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for row in data:\n",
    "        yield row[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.247966Z",
     "start_time": "2022-05-30T01:15:18.236423Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAlRbHul7TZ4",
    "outputId": "1a1c971d-474d-40bc-fccb-d3d6c28780b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<cls>', '<sep>', '<mask>', '<bos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "user_defined_symbols = [\n",
    "    \"<pad>\",\n",
    "    \"<unk>\",\n",
    "    \"<cls>\",\n",
    "    \"<sep>\",\n",
    "    \"<mask>\",\n",
    "    \"<bos>\",\n",
    "    \"<eos>\",\n",
    "]\n",
    "\n",
    "unused_token_num = 100\n",
    "unused_list = [f\"<unused{i}>\" for i in range(unused_token_num)]\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list\n",
    "\n",
    "pprint(user_defined_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M78kc0QyHjW6"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.265314Z",
     "start_time": "2022-05-30T01:15:18.250790Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "gpt_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.269898Z",
     "start_time": "2022-05-30T01:15:18.266633Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFKC, BertNormalizer\n",
    "\n",
    "\n",
    "n1 = NFKC()\n",
    "n2 = BertNormalizer(\n",
    "    clean_text=False,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "gpt_tokenizer.normalizer = normalizers.Sequence([n1, n2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.275783Z",
     "start_time": "2022-05-30T01:15:18.271095Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "\n",
    "gpt_tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [Metaspace(replacement=\"_\", add_prefix_space=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:15:18.281372Z",
     "start_time": "2022-05-30T01:15:18.277298Z"
    }
   },
   "outputs": [],
   "source": [
    "# post_processing pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:43.138485Z",
     "start_time": "2022-05-30T01:15:18.282791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=whole_user_defined_symbols,\n",
    ")\n",
    "gpt_tokenizer.train_from_iterator(gen(), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:43.148022Z",
     "start_time": "2022-05-30T01:17:43.142296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6733, 1667, 3881, 3146, 3703, 233, 216, 1533, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_테스트 용 인데 _잘 _되는 거 같 아'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gpt_tokenizer.encode(\"테스트용인데 잘 되는거같아?\")\n",
    "print(output.ids)\n",
    "\n",
    "gpt_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:43.163177Z",
     "start_time": "2022-05-30T01:17:43.149461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테스트용인데 잘 되는거같아'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "gpt_tokenizer.decoder = decoders.BPEDecoder(suffix=\"_\")\n",
    "gpt_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETFAiEUiHgrG"
   },
   "source": [
    "# convert transformers tokenizer and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:44.329740Z",
     "start_time": "2022-05-30T01:17:43.165209Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jh/.pyenv/versions/3.8.15/envs/jupyter/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "fast_tokenizer = GPT2TokenizerFast(tokenizer_object=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:44.336803Z",
     "start_time": "2022-05-30T01:17:44.331314Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8RmIRpxgnfJ",
    "outputId": "d31adf46-3539-47de-ea74-6e126751e78e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.pad_token = \"<pad>\"\n",
    "fast_tokenizer.unk_token = \"<unk>\"\n",
    "fast_tokenizer.cls_token = \"<cls>\"\n",
    "fast_tokenizer.sep_token = \"<sep>\"\n",
    "fast_tokenizer.mask_token = \"<mask>\"\n",
    "fast_tokenizer.bos_token = \"<bos>\"\n",
    "fast_tokenizer.eos_token = \"<eos>\"\n",
    "\n",
    "special_tokens_dict = {\"additional_special_tokens\": user_defined_symbols}\n",
    "fast_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:47.988542Z",
     "start_time": "2022-05-30T01:17:44.338242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 1, 10283, 3659, 108, 1, 1, 20022, 1667, 1, 2560, 143, 1, 1758, 1, 3703, 1873, 3170, 1734, 751, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 17:29:49.731964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 17:29:49.849025: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-29 17:29:50.393589: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 17:29:50.393654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 17:29:50.393660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>user1<unk><unk>테스트용<unk>으로 <unk>잘<unk> 되는지 보고이따<sep>\n"
     ]
    }
   ],
   "source": [
    "e = fast_tokenizer.encode(\"<user1>'테스트용'으로 \\\"잘\\\" 되는지 보고이따<sep>\")\n",
    "print(e)\n",
    "print(fast_tokenizer.decode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T01:17:48.070946Z",
     "start_time": "2022-05-30T01:17:47.990116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('etc/DialogBPE/tokenizer_config.json',\n",
       " 'etc/DialogBPE/special_tokens_map.json',\n",
       " 'etc/DialogBPE/vocab.json',\n",
       " 'etc/DialogBPE/merges.txt',\n",
       " 'etc/DialogBPE/added_tokens.json',\n",
       " 'etc/DialogBPE/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained(\"etc/DialogBPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HoCW3M3_HeeV",
    "M78kc0QyHjW6",
    "ETFAiEUiHgrG"
   ],
   "name": "evolved_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4086fbbda600ef5a3186756b0a57662a89014dc025df738c34c7b09932df9c3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
